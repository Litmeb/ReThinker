# ReThinker
Decoder-only的llm中，前面的token无法被后面的token影响。换句话说，这个机制假设后续输入不会要求对前缀进行语义上的重解释。然而，很多情况下，后文信息往往会回溯性地改变对前文实体、场景和事件的理解。比方说，给llm看一个故事，一开始所有登场人物都是华人，结果后面提到故事其实发生在美国的唐人街。这个新的信息毫无疑问会影响到对前面内容的理解，如果llm可以自动更新前面的token的词义的话，肯定能捕获更加精准的语义。因此，可以认为模型确实会需要更新kv cache里面的东西。因此，我决定设计一种新的模型结构。它会判断新的信息的惊讶度。如果惊讶度大于阈值，会根据新的信息更新老的token的k、v表示
